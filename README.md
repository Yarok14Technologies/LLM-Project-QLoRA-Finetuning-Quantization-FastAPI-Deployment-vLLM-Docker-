# ğŸ§  **LLM Project â€” QLoRA Finetuning, Quantization & FastAPI Deployment (vLLM + Docker)**

A complete end-to-end pipeline for building your own efficient LLM similar to *Nano/Banana* small models.

This repository includes:

* ğŸŸ© **QLoRA Finetuning** (1Bâ€“7B models)
* ğŸŸ¨ **LoRA Merge + GPTQ/AWQ Quantization**
* ğŸŸ¦ **FastAPI Inference Server (vLLM optimized)**
* ğŸŸ¥ **GPU Deployment with Docker & docker-compose**
* âš™ï¸ **VRAM Model Size Calculator**
* ğŸ“¦ **Modular Repo Structure for Production**

---

# ğŸ“ Repository Structure

```
llm-project/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ data_prep.py
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ finetune_qlora.py
â”‚   â”œâ”€â”€ merge_lora.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ training_args.json
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ lora-output/
â”‚   â”œâ”€â”€ merged-model/
â”‚   â”œâ”€â”€ quantized/
â”‚   â””â”€â”€ tokenizer/
â”‚
â”œâ”€â”€ quantization/
â”‚   â”œâ”€â”€ quantize_gptq.py
â”‚   â”œâ”€â”€ quantize_awq.py
â”‚   â””â”€â”€ calibrate_data.jsonl
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ model_loader.py
â”‚   â”œâ”€â”€ schemas.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ start.sh
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ install_env.sh
â”‚   â”œâ”€â”€ vram_calculator.py
â”‚   â”œâ”€â”€ download_model.py
â”‚   â””â”€â”€ test_api.sh
â”‚
â””â”€â”€ deployment/
    â”œâ”€â”€ nginx.conf
    â”œâ”€â”€ service_monitoring/
    â”œâ”€â”€ systemd_service.service
    â””â”€â”€ k8s/
```

---

# ğŸš€ Quick Start

## 1ï¸âƒ£ **Install Dependencies (local machine)**

```bash
chmod +x scripts/install_env.sh
./scripts/install_env.sh
```

Activating environment:

```bash
source llm-env/bin/activate
```

---

# 2ï¸âƒ£ **Prepare Dataset**

Place JSONL dataset in:

```
data/processed/train.jsonl
```

Format:

```json
{"instruction": "Explain biogas.", "input": "", "output": "Biogas is..."}
```

---

# 3ï¸âƒ£ **Finetune the Model with QLoRA**

Modify your base model path in:

`training/finetune_qlora.py`

Run training:

```bash
python training/finetune_qlora.py
```

Output LoRA adapter is saved to:

```
models/lora-output/
```

---

# 4ï¸âƒ£ **Merge LoRA into Base Model**

```bash
python training/merge_lora.py
```

Merged model saved to:

```
models/merged-model/
```

---

# 5ï¸âƒ£ **Quantize Model (GPTQ)**

```bash
python quantization/quantize_gptq.py \
  --model_path models/merged-model \
  --output_path models/quantized \
  --bits 4
```

Final quantized model is used for API serving.

---

# 6ï¸âƒ£ **Run FastAPI Server (vLLM optimized)**

Ensure correct model path in:

`api/app.py` â†’ `MODEL_PATH="/models/quantized"`

### Local run:

```bash
uvicorn api.app:app --host 0.0.0.0 --port 8000
```

### Test:

```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"Explain biomethane in 2 lines."}'
```

---

# 7ï¸âƒ£ **Deploy with Docker + NVIDIA GPU**

### Build and run:

```bash
cd docker
docker compose up --build -d
```

### Test API:

```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"Hello, what can you do?"}'
```

---

# âš™ï¸ Model Size Calculator

Run:

```bash
python scripts/vram_calculator.py
```

This shows how big a model your GPU can support for:

* FP16
* 8-bit
* 4-bit (GPTQ / AWQ)
* QLoRA finetuning

Includes suggested GPU â†’ model mappings.

---

# ğŸ§© Configuration

### Change base model

`training/finetune_qlora.py`

### Modify training hyperparameters

`training/config/training_args.json`

### Change runtime model path

`api/app.py` â†’ `MODEL_PATH`

---

# ğŸ“¦ Docker Deployment Notes

### Model volume mount:

```
models/quantized â†’ /models/quantized
```

### FastAPI is served via:

```
http://localhost:8000
```

Default endpoints:

| Method | Endpoint    | Description               |
| ------ | ----------- | ------------------------- |
| GET    | `/health`   | Check model health        |
| POST   | `/generate` | Generate text from prompt |

---

# ğŸ”¥ Features

* âš¡ **QLoRA finetuning** (very low VRAM use)
* ğŸ§© **LoRA merging** script included
* âš™ï¸ **GPTQ/AWQ quantization**
* ğŸš€ **vLLM inference** (super-fast GPU serving)
* ğŸ³ **Dockerized deployment with GPU access**
* ğŸ“¡ **FastAPI REST API**
* ğŸ“Š Optional **Prometheus/Grafana monitoring**

---

# ğŸ› ï¸ Hardware Recommendations

| GPU            | Max QLoRA Model | Max Quantized Inference |
| -------------- | --------------- | ----------------------- |
| RTX 3060 12GB  | 1B              | 3B                      |
| RTX 3090 24GB  | 3Bâ€“7B           | 7B                      |
| RTX 4090 24GB  | 7B              | 7B (fast)               |
| A100 40GB      | 13B             | 13B                     |
| A100/H100 80GB | 13Bâ€“34B         | 34B+                    |

---

# ğŸ¤ Contributing

PRs welcome for:

* Training improvements
* LoRA merge automation
* Quantization tools
* Inference optimization
* Docker images

---

# ğŸ“„ License

Apache 2.0 (or your preferred license)

---

# ğŸ’¬ Support

For help integrating your own dataset or customizing inference, open an issue or ask ChatGPT to generate specialized scripts (merge automate, quantize tools, dataset filters, etc.)

---

