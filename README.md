# -LLM-Project-QLoRA-Finetuning-Quantization-FastAPI-Deployment-vLLM-Docker-
End-to-end LLM pipeline: QLoRA finetuning (1Bâ€“7B), LoRA merge, GPTQ/AWQ quantization, FastAPI + vLLM inference server, GPU-ready Docker deployment, VRAM calculator, and a clean, production-grade modular repo structure.
